{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, EarlyStoppingCallback\n",
    "from torch.utils.data import DataLoader\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'labels': tensor([2, 0]), 'input_ids': tensor([[    2,  2564,  1927,  2861,  2259, 29043,  2053,  1990,  5067,  4976,\n",
      "          1922,  2132,  1956,  7529, 21038,  1022, 13821, 18698,    18,  2038,\n",
      "          3620,  2132,  1956,  7529, 21038,  1022, 13821, 18698,  1930,  2285,\n",
      "          3297,  1990,    43,  3909,  1927, 19331,  8228,  4085,  4976,    18,\n",
      "          4845, 16757,  2007,  8972,  1990,  1920,  2841,  4159,  1985, 10266,\n",
      "          3713,  1954, 22701,    31,  1930,  2690,  1990,  1920,  8179,  4159,\n",
      "            16, 27570,  1954, 22701,    18,  1958,  2136,  3297,    16,  3713,\n",
      "          1954, 22701,  4845,  1985, 10711,  1942,  8411,  2254,  2690,  1988,\n",
      "          1985, 27570,  1954, 22701,    18,  1920,  3175,  2192,  3227,  3586,\n",
      "          1958, 27570,  1954,  1930,  3713,  1954, 22701,  4845,    16,  2154,\n",
      "          1977,  7149,  1942,  2931, 13689,  5829,    16,  1982,  2567,  2502,\n",
      "          1958,  1920,  2132,    18,  1920,  3604,  1927,  1920,  3562,  1990,\n",
      "          4908, 14612,  4026,  1982,  4374,  1956,  2310, 13689,  3682,    18,\n",
      "          2144,  6100,  1985,  5520,  2007,  7529, 21038,  1022, 13821, 18698,\n",
      "            18,  2861,  2259, 29043,  2053,  2311,  1927,  1920,  2132,  2719,\n",
      "            43,  5986,  1941,  1927,  3387,  1942, 27570,  1954, 22701,  4845,\n",
      "          1966,    43,  2347,  1927,  1920,  4392,  1927,  1920,  2573,    18,\n",
      "          2144,  2351,  2146,  6007,  1922, 10032,  1930,  4976,  1922,  2132,\n",
      "          1956,  7529, 21038,  1022, 13821, 18698,  1930,  4504,  1988,  2861,\n",
      "          2259, 29043,  2053,  7268,  2673,  1927,  1920,  4552,  3737, 13689,\n",
      "          3604,    18,     3,     0,     0,     0,     0,     0],\n",
      "        [    2,  5373,  1927,  1920,  5771,  8401, 11152,    30,  1925, 15623,\n",
      "            18,  2052,  3043,  1977,    43, 23009,  1927,  1920,  4696,  1927,\n",
      "          3259,  2919,  1927,  5373,  1927,  1920,  5771,  8401, 11152, 10398,\n",
      "          2568,  2007,  1920, 14729,  6799,  1930,  7858,    18, 20461,  3281,\n",
      "          2919,  2162,  2252,  3747,  1930,  2868,  3328, 17380,    18,  8169,\n",
      "          2149,    17,  4885,  6287, 25035,  7415,  2258,  8403, 25276, 13777,\n",
      "         16677,    16,  1930,  4305,  4548, 13682,  5170,  4895,  2162,  5563,\n",
      "          1958,  6091, 24614,  1927,  5570,  4518,    18,    43, 10041,  5492,\n",
      "          1958,  3076,  1930,  6475,  4518,  1977,  3549,    18,  1920,  6573,\n",
      "          1958,  6579,  3861,  2868,  2007,  2066, 30445,  6953,  4497,  7526,\n",
      "            16,  3417,  6573,  1958,  6475,  4518,  3284,  1998,  2253,  3100,\n",
      "          1990, 12434,  2601,  1930,  4927,  1927,  2798,    18,     3,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])})\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### I. Preprocess Data\n",
    "### Balance using up and downsampling create training/validation   \n",
    "\n",
    "DATA_DIR = \"/teamspace/studios/this_studio/data/data_excluding_5\"\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": f\"{DATA_DIR}/medical_tc_train_raw_excl_5.csv\",\n",
    "        \"test\":  f\"{DATA_DIR}/medical_tc_test_raw_excl_5.csv\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# 3) Map labels to 0..3 and cast for stratification\n",
    "ds = ds.map(lambda x: {\"labels\": int(x[\"condition_label\"]) - 1})\n",
    "ds = ds.cast_column(\"labels\", ClassLabel(num_classes=4))\n",
    "\n",
    "# 4) Balance TRAIN with up/down sampling to a common target (mean count)\n",
    "def balance_dataset(hfds, label_col=\"labels\", seed=42, strategy=\"mean\"):\n",
    "    labels = np.array(hfds[label_col])\n",
    "    classes, counts = np.unique(labels, return_counts=True)\n",
    "    if strategy == \"max\":\n",
    "        target = counts.max()\n",
    "    elif strategy == \"min\":\n",
    "        target = counts.min()\n",
    "    elif strategy == \"mean\":\n",
    "        target = int(round(counts.mean()))\n",
    "    else:\n",
    "        target = int(strategy)  # allow int target\n",
    "    rng = np.random.default_rng(seed)\n",
    "    new_idx = []\n",
    "    for c in classes:\n",
    "        idx = np.flatnonzero(labels == c)\n",
    "        if len(idx) > target:    # downsample\n",
    "            sel = rng.choice(idx, size=target, replace=False)\n",
    "        elif len(idx) < target:  # upsample\n",
    "            sel = rng.choice(idx, size=target, replace=True)\n",
    "        else:\n",
    "            sel = idx\n",
    "        new_idx.extend(sel.tolist())\n",
    "    rng.shuffle(new_idx)\n",
    "    return hfds.select(new_idx)\n",
    "\n",
    "balanced_train_raw = balance_dataset(ds[\"train\"], label_col=\"labels\", seed=42, strategy=\"mean\")\n",
    "\n",
    "# 5) Split balanced TRAIN -> 80/20 (train/val), stratified\n",
    "split = balanced_train_raw.train_test_split(test_size=0.2, stratify_by_column=\"labels\", seed=42)\n",
    "train_raw = split[\"train\"]\n",
    "val_raw   = split[\"test\"]\n",
    "test_raw  = ds[\"test\"]        # test kept as-is (but class 5 already removed)\n",
    "\n",
    "# 6) Tokenize AFTER split\n",
    "tok = AutoTokenizer.from_pretrained(\"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\")\n",
    "\n",
    "def tokenize_fn(b):\n",
    "    return tok(b[\"medical_abstract\"], truncation=True, max_length=512)\n",
    "\n",
    "cols_to_remove = [\"medical_abstract\", \"condition_label\"]\n",
    "\n",
    "train_ds = train_raw.map(tokenize_fn, batched=True, remove_columns=cols_to_remove)\n",
    "val_ds   = val_raw.map(tokenize_fn,   batched=True, remove_columns=cols_to_remove)\n",
    "test_ds  = test_raw.map(tokenize_fn,  batched=True, remove_columns=cols_to_remove)\n",
    "\n",
    "# 7) Collator (Tensor Core-friendly)\n",
    "collator = DataCollatorWithPadding(tok, pad_to_multiple_of=8)\n",
    "\n",
    "# quick sanity check\n",
    "batch = next(iter(DataLoader(train_ds, batch_size=2, collate_fn=collator)))\n",
    "print(batch.keys()); print(batch[\"labels\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.2\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "##################################################\n",
    "#### Model Initialization\n",
    "########################################################\n",
    "\n",
    "# Initialize a BERT model for multilabel classification\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=4)\n",
    "\n",
    "print(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "##### Definining several metrics to compare model quality\n",
    "##################### \n",
    "\n",
    "acc = load(\"accuracy\")\n",
    "f1  = load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1_macro\": f1.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"],\n",
    "        \"f1_weighted\": f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fine tune whole model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#################################\\n#### Fine Tune whole model Training config\\n########################\\n\\n\\ntraining_args = TrainingArguments(\\n    output_dir=\"./results\",\\n    eval_strategy=\"epoch\",\\n    save_strategy=\"epoch\",\\n    learning_rate=2e-5,\\n    per_device_train_batch_size=32,   \\n    per_device_eval_batch_size=32,\\n    num_train_epochs=10,\\n    weight_decay=0.1,\\n    warmup_ratio=0.06,\\n    lr_scheduler_type=\"linear\",\\n    \\n    seed=42,\\n    save_total_limit=2,\\n    load_best_model_at_end=True,\\n    logging_dir=\"./logs\",\\n    logging_steps=100,\\n    fp16=True,                  \\n    metric_for_best_model=\"eval_loss\",\\n    greater_is_better=False\\n)\\n\\ntrainer = Trainer(\\n    model=model,                        # Pre-trained BERT model\\n    args=training_args,                 # Training arguments\\n    train_dataset=train_ds,             \\n    eval_dataset=val_ds,\\n    tokenizer=tok,\\n    data_collator=collator,        # Efficient batching\\n    compute_metrics=compute_metrics,   # Custom metric\\n    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\\n)\\n\\n# Start training\\ntrainer.train()\\n\\n#Evaluate on Test Set\\ntrainer.evaluate(test_ds)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''#################################\n",
    "#### Fine Tune whole model Training config\n",
    "########################\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,   \n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.1,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    \n",
    "    seed=42,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,                  \n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                        # Pre-trained BERT model\n",
    "    args=training_args,                 # Training arguments\n",
    "    train_dataset=train_ds,             \n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,        # Efficient batching\n",
    "    compute_metrics=compute_metrics,   # Custom metric\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "#Evaluate on Test Set\n",
    "trainer.evaluate(test_ds)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attempt**\n",
    "***Train only classifier Head***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,076 / 109,485,316\n"
     ]
    }
   ],
   "source": [
    "####################  \n",
    "## Training only classifier head, freeze all the other parameters\n",
    "# Uncomment when training whole model\n",
    "####################\n",
    "\n",
    "# 1) freeze the encoder (backbone)\n",
    "for p in model.bert.parameters():         # BiomedBERT is a BERT; backbone is `model.bert`\n",
    "    p.requires_grad = False\n",
    "\n",
    "# 2) keep the classifier trainable\n",
    "for p in model.classifier.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# (optional) sanity check\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total     = sum(p.numel() for p in model.parameters())\n",
    "print(f\"trainable params: {trainable:,} / {total:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4579/2909877939.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(  # or your WeightedTrainer\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3860' max='3860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3860/3860 18:06, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "      <th>F1 Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.115800</td>\n",
       "      <td>1.093496</td>\n",
       "      <td>0.663855</td>\n",
       "      <td>0.661903</td>\n",
       "      <td>0.661909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.090200</td>\n",
       "      <td>1.046268</td>\n",
       "      <td>0.687865</td>\n",
       "      <td>0.687981</td>\n",
       "      <td>0.687979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.043000</td>\n",
       "      <td>1.003937</td>\n",
       "      <td>0.699546</td>\n",
       "      <td>0.699617</td>\n",
       "      <td>0.699621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.008500</td>\n",
       "      <td>0.976388</td>\n",
       "      <td>0.714471</td>\n",
       "      <td>0.713400</td>\n",
       "      <td>0.713406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>0.949310</td>\n",
       "      <td>0.722907</td>\n",
       "      <td>0.722808</td>\n",
       "      <td>0.722813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.971800</td>\n",
       "      <td>0.936698</td>\n",
       "      <td>0.729396</td>\n",
       "      <td>0.729414</td>\n",
       "      <td>0.729412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.959800</td>\n",
       "      <td>0.921106</td>\n",
       "      <td>0.738482</td>\n",
       "      <td>0.738242</td>\n",
       "      <td>0.738250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.938600</td>\n",
       "      <td>0.910657</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.739184</td>\n",
       "      <td>0.739192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.930100</td>\n",
       "      <td>0.902051</td>\n",
       "      <td>0.741726</td>\n",
       "      <td>0.741766</td>\n",
       "      <td>0.741773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.919500</td>\n",
       "      <td>0.896039</td>\n",
       "      <td>0.743673</td>\n",
       "      <td>0.743377</td>\n",
       "      <td>0.743388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.932500</td>\n",
       "      <td>0.891813</td>\n",
       "      <td>0.750811</td>\n",
       "      <td>0.751141</td>\n",
       "      <td>0.751149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.913700</td>\n",
       "      <td>0.887275</td>\n",
       "      <td>0.750162</td>\n",
       "      <td>0.750206</td>\n",
       "      <td>0.750209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.916700</td>\n",
       "      <td>0.879901</td>\n",
       "      <td>0.754056</td>\n",
       "      <td>0.753996</td>\n",
       "      <td>0.754001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.901900</td>\n",
       "      <td>0.879026</td>\n",
       "      <td>0.746918</td>\n",
       "      <td>0.747053</td>\n",
       "      <td>0.747064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.908900</td>\n",
       "      <td>0.876336</td>\n",
       "      <td>0.752758</td>\n",
       "      <td>0.752873</td>\n",
       "      <td>0.752880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.872958</td>\n",
       "      <td>0.752109</td>\n",
       "      <td>0.752117</td>\n",
       "      <td>0.752122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.897800</td>\n",
       "      <td>0.872177</td>\n",
       "      <td>0.751460</td>\n",
       "      <td>0.751466</td>\n",
       "      <td>0.751475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.898500</td>\n",
       "      <td>0.869838</td>\n",
       "      <td>0.755354</td>\n",
       "      <td>0.755408</td>\n",
       "      <td>0.755416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.893500</td>\n",
       "      <td>0.869329</td>\n",
       "      <td>0.757300</td>\n",
       "      <td>0.757268</td>\n",
       "      <td>0.757274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.891300</td>\n",
       "      <td>0.869061</td>\n",
       "      <td>0.754705</td>\n",
       "      <td>0.754658</td>\n",
       "      <td>0.754664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [61/61 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8576733469963074,\n",
       " 'eval_accuracy': 0.7586922677737415,\n",
       " 'eval_f1_macro': 0.7414171167519136,\n",
       " 'eval_f1_weighted': 0.7611282720361802,\n",
       " 'eval_runtime': 12.4216,\n",
       " 'eval_samples_per_second': 155.133,\n",
       " 'eval_steps_per_second': 4.911,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################### \n",
    "###### Head only Trainer Config\n",
    "########################\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_head_only\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-4,              # 5e-4 ~ 1e-3 typical; lower if unstable\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,              # ES will stop earlier if needed\n",
    "    weight_decay=0.0,               \n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    label_smoothing_factor=0.1,      # optional regularization\n",
    "    seed=42,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    fp16=True,                       # fine on T4; if loss blows up, try 5e-4 or fp16=False\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(  # or your WeightedTrainer\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tok,\n",
    "    data_collator=DataCollatorWithPadding(tok, pad_to_multiple_of=8),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate(test_ds)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
